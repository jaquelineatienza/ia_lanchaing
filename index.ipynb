{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b257b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "document = PyPDFLoader(\"rag.pdf\")\n",
    "\n",
    "loader = document.lazy_load()\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "for page in loader:\n",
    "    text += page.page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f45e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=100,\n",
    "    separator=\"\\n\" \n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11280cc5",
   "metadata": {},
   "source": [
    "#embedding echo por nomic-embed-text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "198b8c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embedding = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e48454",
   "metadata": {},
   "source": [
    "#embedding echo por todo-minilm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d8ca963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma \n",
    "\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"test\",\n",
    "    embedding_function=embedding,\n",
    "    persist_directory=\"./vectorStore-nomic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db0520d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['248c0941-3395-411a-8f74-893c107c8ac5']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    api_key=api_key,\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    temperature=0.5\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "422f52df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "ai_msg = [\"\"]\n",
    "human_msg = [ ]\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\",\"\"\"\n",
    "Eres un asistente encargado de responder preguntas sobre\n",
    "Langchain.\n",
    "Responde solo si {context} posee contenido. Si\n",
    "el contexto esta vacio,\n",
    "responde \"No tengo suficiente informacion para\n",
    "reponder esa pregunta\"\n",
    "\"\"\"),\n",
    "(\"ai\",\"{ai_msg}\"),\n",
    "(\"human\",\"{human_msg}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "89e2b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_user =input(\"Human: \")\n",
    "human_msg.append(input_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eb66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vector_store.similarity_search(input_user, k=10,\n",
    "                                         filter={\"page\": 10} )\n",
    "\n",
    "prompt= prompt_template.invoke({\n",
    "    \"context\":docs,\n",
    "    \"ai_msg\":ai_msg,\n",
    "    \"human_msg\":human_msg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d6f5bae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tecnología RAG (Generación Aumentada por Recuperación) aporta varios beneficios a los esfuerzos de la IA generativa de una organización:\n",
      "\n",
      "*   **Implementación rentable**: Es un enfoque más económico para introducir nuevos datos en un LLM sin necesidad de volver a entrenar el modelo fundacional, haciendo la IA generativa más accesible.\n",
      "*   **Información actual**: Permite a los desarrolladores proporcionar las últimas investigaciones, estadísticas o noticias a los modelos generativos, conectando el LLM a fuentes de información que se actualizan con frecuencia.\n",
      "*   **Mayor confianza de los usuarios**: El LLM puede presentar información precisa con atribución de la fuente, incluyendo citas o referencias, lo que aumenta la confianza del usuario.\n",
      "*   **Más control para los desarrolladores**: Los desarrolladores pueden probar y mejorar sus aplicaciones de chat de manera más eficiente, controlando y cambiando las fuentes de información, restringiendo la recuperación de datos confidenciales y solucionando problemas si el LLM hace referencia a fuentes incorrectas.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(prompt)\n",
    "print(response.content)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
