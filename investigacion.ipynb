{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed36b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "document = PyPDFLoader(\"rag.pdf\")\n",
    "\n",
    "loader = document.lazy_load()\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "for page in loader:\n",
    "    text += page.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43a79858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=100,\n",
    "    separator=\"\\n\" \n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f422df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embedding = OllamaEmbeddings(model=\"all-minilm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ab0cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Crear el índice FAISS desde los documentos\n",
    "vector_store = FAISS.from_documents(texts, embedding)\n",
    "\n",
    "# Guardarlo en disco\n",
    "vector_store.save_local(\"./vectorStore-pdf-faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb67ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.load_local(\n",
    "    \"./vectorStore-pdf-faiss\",\n",
    "    embeddings=embedding,\n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "368d21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\", temperature=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f65ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ai_msg = [\"\"]\n",
    "human_msg = []\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"\"\"\n",
    "Eres un asistente especializado en LangChain con acceso a documentación técnica actualizada.\n",
    "\n",
    "INSTRUCCIONES ESPECÍFICAS:\n",
    "1. **Análisis del contexto**: Examina cuidadosamente el {context} proporcionado\n",
    "2. **Atribución de fuentes**: Cuando uses información del contexto, menciona específicamente de qué parte proviene\n",
    "3. **Honestidad intelectual**: Si el contexto no contiene información suficiente, indica claramente \"No tengo suficiente información para responder esa pregunta basándome en el contexto proporcionado\"\n",
    "4. **Estructura de respuesta**:\n",
    "   - Comienza con un resumen conciso\n",
    "   - Proporciona detalles técnicos relevantes\n",
    "   - Incluye ejemplos prácticos cuando sea posible\n",
    "   - Menciona casos de uso aplicables\n",
    "5. **Evitar especulación**: No inventes características o funcionalidades no presentes en el contexto\n",
    "\n",
    "\n",
    "\n",
    "Contexto disponible:\n",
    "{context}\n",
    "\"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cffa9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_user = input(\"Human: \")\n",
    "human_msg.append(input_user)\n",
    "\n",
    "results = vector_store.similarity_search(input_user, k=3)\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b43fcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tecnología RAG (generación aumentada por recuperación) aporta varios beneficios a los esfuerzos de la IA generativa de una organización.\n",
      "\n",
      " ● Implementación rentable: El desarrollo de chatbots normalmente comienza con un modelo fundamental. Los modelos fundamentales (FM) son LLM accesibles para API entrenados en un amplio espectro de datos generalizados y sin etiquetar. Los costos computacionales y financieros de volver a entrenar los FM para obtener información específica de la organización son altos. La RAG es un enfoque más rentable para introducir nuevos datos en el LLM.\n",
      "\n",
      " ● Mayor control para los desarrolladores: Con la RAG, los desarrolladores pueden probar y mejorar sus aplicaciones de chat de manera más eficiente. Pueden controlar y cambiar las fuentes de información del LLM para adaptarse a los requisitos cambiantes y multifuncionales. Los desarrolladores también pueden restringir la recuperación de información confidencial a diferentes niveles de autorización y garantizar que el LLM genere respuestas adecuadas.\n",
      "\n",
      " ● Mayor confianza en los usuarios: La RAG le permite al LLM presentar información precisa con atribución de la fuente. La salida puede incluir citas y referencias a fuentes. Los usuarios también pueden buscar los mismos documentos de origen si necesitan más aclaraciones o detalles. Esto puede aumentar la confianza en las soluciones de IA generativa.\n",
      "\n",
      " ● Mejora de la relevancia: La RAG extiende las poderosas capacidades de los LLM a dominios específicos sin la necesidad de volver a entrenar el modelo. Hace que la tecnología de inteligencia artificial sea más accesible y rentable para una amplia gama de aplicaciones.\n",
      "\n",
      "En resumen, la generación aumentada por recuperación (RAG) es un enfoque innovador que puede mejorar significativamente los resultados de las soluciones de IA generativa, permitiendo a los desarrolladores implementar modelos más eficientes y confiables.\n"
     ]
    }
   ],
   "source": [
    "# 8. Invocar modelo\n",
    "prompt = prompt_template.invoke({\n",
    "    \"context\": results,\n",
    "    \"ai_msg\": ai_msg,\n",
    "    \"human_msg\": human_msg\n",
    "})\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
